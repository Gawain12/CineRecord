import requests
import csv
import time
import random
import json
import os
import re
import sys
from datetime import datetime
import pandas as pd
from tqdm import tqdm
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from config.config import IMDB_CONFIG, DOUBAN_CONFIG

class IMDbRatingsScraper:
    """
    Fetches IMDb user ratings incrementally and enriches them with Douban IDs.
    """
    def __init__(self):
        # Load Config
        self.user_id = IMDB_CONFIG.get('user_id')
        self.imdb_headers = IMDB_CONFIG.get('headers', {})
        self.douban_headers = DOUBAN_CONFIG.get('headers', {})

        # Set default User-Agent if not provided to avoid 403 errors
        default_user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36'
        if 'User-Agent' not in self.imdb_headers:
            self.imdb_headers['User-Agent'] = default_user_agent
        if 'User-Agent' not in self.douban_headers:
            self.douban_headers['User-Agent'] = default_user_agent

        if not self.user_id or not self.imdb_headers.get('Cookie') or not self.douban_headers.get('Cookie'):
            raise SystemExit("‚ùå Config Error: Ensure user_id and cookies are set for both IMDb and Douban.")

        print("‚úÖ ÊàêÂäüÂä†ËΩΩ IMDb ÂíåË±ÜÁì£ÁöÑÈÖçÁΩÆ„ÄÇ")

        # Define base paths
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

        # Filenames and URLs
        self.output_filename = os.path.join(self.project_root, "data", f"imdb_{self.user_id}_ratings.csv")
        self.api_base_url = "https://api.graphql.imdb.com/"
        self.web_base_url = f"https://www.imdb.com/user/{self.user_id}/ratings"
        self.douban_search_url = "https://m.douban.com/rexxar/api/v2/search"

        # Session
        self.session = requests.Session()

        # Caching
        self.douban_id_cache_file = os.path.join(self.project_root, "data", "db_imdb.csv")
        self.douban_id_cache = self._load_douban_id_cache()
        self.newly_found_mappings = {}
        print(f"‚úÖ Â∑≤‰ªé‰∏≠Â§ÆÁºìÂ≠ò ('{self.douban_id_cache_file}') Âä†ËΩΩ {len(self.douban_id_cache)} Êù° IMDb-Ë±ÜÁì£ ID Êò†Â∞Ñ„ÄÇ")
        print(f"‚ÑπÔ∏è  ËæìÂá∫ÁöÑ CSV Êñá‰ª∂Â∞Ü‰øùÂ≠òËá≥: {self.output_filename}")

        # Load existing ratings for incremental check
        self.existing_imdb_ids = self._load_existing_ratings()
        print(f"‚úÖ Âú® '{self.output_filename}' ‰∏≠ÂèëÁé∞ {len(self.existing_imdb_ids)} Êù°Áé∞ÊúâÁîµÂΩ±ËØÑÂàÜ„ÄÇ")


    def _load_existing_ratings(self):
        if not os.path.exists(self.output_filename):
            return set()
        try:
            df = pd.read_csv(self.output_filename, usecols=['Const'], dtype={'Const': str})
            return set(df['Const'].dropna())
        except (pd.errors.ParserError, pd.errors.EmptyDataError, KeyError, FileNotFoundError):
            return set()

    def _load_douban_id_cache(self):
        if not os.path.exists(self.douban_id_cache_file):
            return {}
        try:
            df = pd.read_csv(self.douban_id_cache_file, dtype=str)
            # Standardize column name for douban id (could be 'id' or 'douban_id')
            if 'douban_id' not in df.columns and 'id' in df.columns:
                df.rename(columns={'id': 'douban_id'}, inplace=True)

            # Ensure required columns exist before proceeding
            if 'douban_id' not in df.columns or 'imdb' not in df.columns:
                print(f"‚ö†Ô∏è Ë≠¶Âëä: ÁºìÂ≠òÊñá‰ª∂ '{self.douban_id_cache_file}' Áº∫Â∞ë 'douban_id' Êàñ 'imdb' Âàó„ÄÇ")
                return {}
            
            df.dropna(subset=['douban_id', 'imdb'], inplace=True)
            # Create a mapping from imdb_id -> douban_id for direct lookup
            return pd.Series(df.douban_id.values, index=df.imdb).to_dict()
        except (pd.errors.ParserError, pd.errors.EmptyDataError) as e:
            print(f"‚ö†Ô∏è Ë≠¶Âëä: ÁºìÂ≠òÊñá‰ª∂ '{self.douban_id_cache_file}' ÂèØËÉΩÂ∑≤ÊçüÂùèÔºåÂ∞ÜÈáçÊñ∞ÂºÄÂßã„ÄÇÂéüÂõ†: {e}")
            return {}

    def _save_newly_found_mappings(self):
        if not self.newly_found_mappings:
            return

        print(f"\\n‚úçÔ∏è  Ê≠£Âú®Â∞Ü {len(self.newly_found_mappings)} Êù°Êñ∞ÂèëÁé∞ÁöÑÊò†Â∞ÑÂÖ≥Á≥ªÊõ¥Êñ∞Âà∞‰∏≠Â§ÆÁºìÂ≠òÊñá‰ª∂: {self.douban_id_cache_file}")

        # Convert the newly found mappings (imdb -> douban) to a DataFrame
        new_mappings_df = pd.DataFrame(list(self.newly_found_mappings.items()), columns=['imdb', 'douban_id'])

        # Read the existing cache file
        try:
            if os.path.exists(self.douban_id_cache_file):
                existing_df = pd.read_csv(self.douban_id_cache_file, dtype=str)
                # Standardize column names
                if 'douban_id' not in existing_df.columns and 'id' in existing_df.columns:
                    existing_df.rename(columns={'id': 'douban_id'}, inplace=True)
            else:
                existing_df = pd.DataFrame(columns=['douban_id', 'imdb'])
        except pd.errors.EmptyDataError:
            existing_df = pd.DataFrame(columns=['douban_id', 'imdb'])

        # Combine old and new mappings
        combined_df = pd.concat([existing_df, new_mappings_df], ignore_index=True)
        
        # Remove duplicates, keeping the most recent entry for any given ID
        combined_df.drop_duplicates(subset=['imdb'], keep='last', inplace=True)
        combined_df.drop_duplicates(subset=['douban_id'], keep='last', inplace=True)

        # Save the updated cache
        combined_df.to_csv(self.douban_id_cache_file, index=False, encoding='utf-8')

    def _fetch_api_page(self, cursor):
        payload = {"operationName": "userRatings", "variables": {"first": 250, "after": cursor},"extensions": {"persistedQuery": {"version": 1, "sha256Hash": "ebf2387fd2ba45d62fc54ed2ffe3940086af52e700a1b3929a099d5fce23330a"}}}
        try:
            response = self.session.post(self.api_base_url, json=payload, headers=self.imdb_headers, timeout=30)
            response.raise_for_status(); return response.json()
        except requests.RequestException as e: print(f"‚ùå API request failed: {e}"); return None

    def _fetch_web_page(self, page_num):
        url = f"{self.web_base_url}?sort=date_added,desc&page={page_num}"
        try:
            response = self.session.get(url, headers=self.imdb_headers, timeout=30)
            response.raise_for_status(); return response.text
        except requests.RequestException as e: print(f"‚ùå Web page request failed: {e}"); return None

    def _fetch_all_personal_ratings(self):
        print("\nüöÄ Ê≠£Âú®‰ªé API Ëé∑ÂèñÊâÄÊúâ‰∏™‰∫∫ËØÑÂàÜ...")
        personal_data_map = {}
        cursor = None
        with tqdm(desc="Ëé∑Âèñ API È°µÈù¢", unit="È°µ") as pbar:
            while True:
                api_data = self._fetch_api_page(cursor)
                if not api_data or api_data.get("errors"): break
                ratings_data = api_data.get('data', {}).get('userRatings', {})
                if not ratings_data or not ratings_data.get('edges'): break
                for edge in ratings_data['edges']:
                    node = edge.get('node', {}); imdb_id = node.get('title', {}).get('id')
                    if not imdb_id: continue
                    ur = node.get('userRating', {})
                    rating_date_raw = ur.get('date')
                    personal_data_map[imdb_id] = {
                        'my_rating': ur.get('value'),
                        'rating_date': datetime.fromisoformat(rating_date_raw.replace('Z', '+00:00')).strftime('%Y-%m-%d') if rating_date_raw else None
                    }
                page_info = ratings_data.get('pageInfo', {})
                if page_info.get('hasNextPage'):
                    cursor = page_info.get('endCursor'); pbar.update(1); pbar.set_postfix(total_ratings=len(personal_data_map))
                else: break
        print(f"‚úÖ API Ëé∑ÂèñÂÆåÊàê„ÄÇÂÖ±ÊâæÂà∞ {len(personal_data_map)} Êù°‰∏™‰∫∫ËØÑÂàÜ„ÄÇ")
        return personal_data_map

    def _fetch_douban_id(self, imdb_id):
        # The cache is now a direct mapping of {imdb_id: douban_id}
        if imdb_id in self.douban_id_cache:
            return self.douban_id_cache[imdb_id]

        if imdb_id in self.newly_found_mappings: 
            return self.newly_found_mappings[imdb_id]

        time.sleep(random.uniform(0.5, 2.0))
        params = {'q': imdb_id, 'type': 'movie', 'count': 1}
        try:
            response = self.session.get(self.douban_search_url, params=params, headers=self.douban_headers, timeout=20, verify=False)
            response.raise_for_status(); data = response.json()
            subjects = data.get('subjects')
            if subjects and isinstance(subjects, list) and subjects:
                douban_id = subjects[0].get('target_id')
                if douban_id:
                    print(f"  - [API] ÂèëÁé∞Êñ∞Êò†Â∞Ñ: IMDb {imdb_id} -> Ë±ÜÁì£ {douban_id}")
                    self.newly_found_mappings[imdb_id] = douban_id
                    # Also update the main cache in memory immediately
                    self.douban_id_cache[imdb_id] = douban_id
                    return douban_id
            return None
        except requests.RequestException as e: 
            print(f"‚ùå Douban ID fetch for {imdb_id} failed: {e}")
            return None

    def _parse_movie_details_from_node(self, node):
        details = {}; title_info = node.get('title', {}) or {}
        details['imdb_id'] = title_info.get('id');
        if not details['imdb_id']: return None
        details['title'] = (title_info.get('titleText', {}) or {}).get('text'); details['year'] = (title_info.get('releaseYear', {}) or {}).get('year')
        details['title_type'] = (title_info.get('titleType', {}) or {}).get('text')
        release_date_obj = title_info.get('releaseDate', {}) or {}
        if all(k in release_date_obj for k in ['year', 'month', 'day']):
            try: details['release_date'] = f"{release_date_obj['year']}-{str(release_date_obj['month']).zfill(2)}-{str(release_date_obj['day']).zfill(2)}"
            except TypeError: details['release_date'] = None
        else: details['release_date'] = None
        ratings_summary = title_info.get('ratingsSummary', {}) or {}; details['imdb_rating'] = ratings_summary.get('aggregateRating'); details['imdb_votes'] = ratings_summary.get('voteCount')
        runtime = title_info.get('runtime', {}) or {}; details['runtime_minutes'] = (runtime.get('seconds', 0) or 0) // 60
        details['genres'] = ', '.join([g.get('genre', {}).get('text', '') for g in (title_info.get('titleGenres', {}) or {}).get('genres', []) or []])
        credits_list = title_info.get('principalCredits', []) or []; directors = []
        for credit_category in credits_list:
            if (credit_category.get('category', {}) or {}).get('id') == 'director':
                directors.extend([(c.get('name', {}) or {}).get('nameText', {}).get('text', '') for c in credit_category.get('credits', []) or []])
        details['director'] = ', '.join(directors)
        return details

    def scrape_all_ratings(self):
        personal_data_map = self._fetch_all_personal_ratings()
        newly_scraped_movies = []
        page_num = 1
        stop_scraping = False
        seen_on_previous_pages = set()
        print("\nüöÄ Ê≠£Âú®‰ªéÁΩëÈ°µÂ¢ûÈáèËé∑ÂèñÊâÄÊúâÂÖ¨ÂºÄÁîµÂΩ±ËØ¶ÊÉÖ...")
        with tqdm(desc="Ëé∑ÂèñÁΩëÈ°µ", unit="È°µ") as pbar:
            while not stop_scraping:
                html_content = self._fetch_web_page(page_num)
                if not html_content: break
                match = re.search(r'<script id="__NEXT_DATA__" type="application/json">(.+?)</script>', html_content, re.DOTALL)
                if not match: break
                
                data = json.loads(match.group(1)); search_results = data['props']['pageProps']['mainColumnData']['advancedTitleSearch']
                movies_on_page = [self._parse_movie_details_from_node(edge.get('node', {})) for edge in search_results.get('edges', [])]
                movies_on_page = [m for m in movies_on_page if m]

                # --- Loop Termination Check ---
                # If all movies on this page have been seen before, it's a duplicate page.
                current_page_ids = {movie['imdb_id'] for movie in movies_on_page if movie.get('imdb_id')}
                if current_page_ids and current_page_ids.issubset(seen_on_previous_pages):
                    print(f"\n[Á¨¨ {page_num} È°µ] Ê£ÄÊµãÂà∞ÈáçÂ§çÈ°µÈù¢„ÄÇÊâÄÊúâÁîµÂΩ±ÈÉΩÂ∑≤ËßÅËøá„ÄÇÊ≠£Âú®ÂÅúÊ≠¢„ÄÇ")
                    break
                seen_on_previous_pages.update(current_page_ids)
                # --------------------------------

                if not movies_on_page: break

                print(f"\n[Á¨¨ {page_num} È°µ] ÊâæÂà∞ {len(movies_on_page)} ÈÉ®ÁîµÂΩ±„ÄÇÊ≠£Âú®Â§ÑÁêÜ...")
                for movie in tqdm(movies_on_page, desc=f"Á¨¨ {page_num} È°µ", leave=False):
                    if movie['imdb_id'] in self.existing_imdb_ids:
                        print(f"\n‚úÖ ÂèëÁé∞Â∑≤Â≠òÂú®ÁöÑÁîµÂΩ± ({movie['imdb_id']}: {movie['title']})„ÄÇÊ≠£Âú®ÂÅúÊ≠¢Â¢ûÈáèÊäìÂèñ„ÄÇ")
                        stop_scraping = True; break
                    
                    movie['douban_id'] = self._fetch_douban_id(movie['imdb_id'])
                    if movie['imdb_id'] in personal_data_map: movie.update(personal_data_map[movie['imdb_id']])
                    newly_scraped_movies.append(movie)

                if not stop_scraping: page_num += 1; pbar.update(1); pbar.set_postfix(new_movies=len(newly_scraped_movies))

        return newly_scraped_movies

def main():
    # Unset proxy environment variables to prevent connection errors
    os.environ.pop('HTTP_PROXY', None)
    os.environ.pop('HTTPS_PROXY', None)

    start_time = time.time()
    print("="*60 + "\n" + " IMDb Â¢ûÈáèËØÑÂàÜÊäìÂèñÂ∑•ÂÖ∑ ".center(60) + "\n" + "="*60)
    try:
        scraper = IMDbRatingsScraper()
        newly_scraped_movies = scraper.scrape_all_ratings()
        scraper._save_newly_found_mappings()

        if not newly_scraped_movies:
            print("\n‚úÖ Êú™ÂèëÁé∞Êñ∞ÁîµÂΩ±„ÄÇÊÇ®ÁöÑÊú¨Âú∞Êñá‰ª∂Â∑≤ÊòØÊúÄÊñ∞„ÄÇ")
            return

        print(f"\n‚úÖ ÊäìÂèñÂà∞ {len(newly_scraped_movies)} ÈÉ®Êñ∞ÁîµÂΩ±„ÄÇ")
        
        export_fieldnames = ['Const', 'Your Rating', 'Date Rated', 'Title', 'URL', 'Title Type', 'IMDb Rating', 'Runtime (mins)', 'Year', 'Genres', 'Num Votes', 'Release Date', 'Directors', 'douban_id']
        export_data = []
        for movie in newly_scraped_movies:
            export_data.append({
                'Const': movie.get('imdb_id'), 'Your Rating': movie.get('my_rating'), 'Date Rated': movie.get('rating_date'),
                'Title': movie.get('title'), 'URL': f"https://www.imdb.com/title/{movie.get('imdb_id')}/",
                'Title Type': movie.get('title_type'), 'IMDb Rating': movie.get('imdb_rating'), 'Runtime (mins)': movie.get('runtime_minutes'),
                'Year': movie.get('year'), 'Genres': movie.get('genres'), 'Num Votes': movie.get('imdb_votes'),
                'Release Date': movie.get('release_date'), 'Directors': movie.get('director'),
                'douban_id': movie.get('douban_id')
            })

        df_new = pd.DataFrame(export_data)
        
        if os.path.exists(scraper.output_filename):
            print(f"üìñ Ê≠£Âú®‰ªé {scraper.output_filename} ËØªÂèñÁé∞ÊúâÊï∞ÊçÆ‰ª•ËøõË°åÂêàÂπ∂...")
            df_existing = pd.read_csv(scraper.output_filename)
            df_combined = pd.concat([df_new, df_existing], ignore_index=True)
        else:
            df_combined = df_new

        df_final = df_combined.drop_duplicates(subset=['Const'], keep='first')
        df_final.sort_values(by='Date Rated', ascending=False, inplace=True)
        df_final.to_csv(scraper.output_filename, index=False, columns=export_fieldnames, encoding='utf-8-sig')

        print(f"üéâ ÊàêÂäüÔºÅÂÖ±‰øùÂ≠ò {len(df_final)} Êù°ËÆ∞ÂΩïÂà∞: {scraper.output_filename}")

    except SystemExit as e:
        print(f"\nËÑöÊú¨ÁªàÊ≠¢: {e}")
    except Exception as e:
        import traceback; print(f"\nÂèëÁîüÊÑèÂ§ñÈîôËØØ: {e}"); traceback.print_exc()
    finally:
        duration = time.time() - start_time
        if duration > 1.0:
            print("\n" + "="*60 + f"\nÁ®ãÂ∫èÂú® {duration:.2f} ÁßíÂÜÖÂÆåÊàê„ÄÇ\n" + "="*60)

if __name__ == "__main__":
    main()
